{
  "8": {
    "inputs": {
      "seed": [
        "129",
        0
      ],
      "steps": 25,
      "cfg": 7.0200000000000005,
      "sampler_name": "dpmpp_2m",
      "scheduler": "karras",
      "denoise": 1,
      "model": [
        "16",
        0
      ],
      "positive": [
        "121",
        0
      ],
      "negative": [
        "121",
        1
      ],
      "latent_image": [
        "120",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler(SB)"
    }
  },
  "9": {
    "inputs": {
      "ckpt_name": "beautifulRealistic_brav5.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "11": {
    "inputs": {
      "text": "(white dress:1.4), (luxury wedding dress:1.3), bride,long dress,elegant floor-length skirt,(korean1.2), Best Quality, (Masterpiece:1.4), (Realism:1.2), (Realisitc:1.2), 4K,(photorealistic:1.3), Detailed, 1 girl, beautiful and pretty girl, (black background: 1.3), photo-realistic, realism, finely detail, Super Resolution, super detail, ultra-detailed, ultra high res, RAW photo, detailed cafe, perfect fingers, beautiful face, beautiful detailed eyes, symmetric eyes",
      "clip": [
        "16",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "Positive Prompt(SB)"
    }
  },
  "12": {
    "inputs": {
      "text": "(black clothes:1.5), (black dress:1.5), (black accessories), text, paintings,sketches,worst quality,low quality,easynegative,lowres,bad hands,bad anatomy,poorly drawn hands,poorly drawn face, extra digit,fewer digits,signature,blurry, bad feet, fused girls, username,duplicate,morbid,mutilated, mutated hands,mutation, deformed, bad proportions, malformed limbs, extra limbs, disfigured, gross proportions,disfigured, bad art, deformed,plump, bad anatomy, error, watermark,long neck, long torso, bad feet, pubic hair, extra digit bad legs,missing legs,missing arms, bad anatomy, wrong finger,missing fingers,big breasts,huge breasts,large breasts,bad hands, extra arms,extra hands,ugly face,cloned face, deformed face,malformed face, extra head,badhandv4,multiple view, multiple views,bad_pictures,logo,",
      "clip": [
        "16",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "Negative Prompt(SB)"
    }
  },
  "13": {
    "inputs": {
      "samples": [
        "8",
        0
      ],
      "vae": [
        "15",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "15": {
    "inputs": {
      "vae_name": "vae-ft-mse-840000-ema-pruned.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "16": {
    "inputs": {
      "lora_name": "Wedding dressV1.safetensors",
      "strength_model": 0.8,
      "strength_clip": 1,
      "model": [
        "9",
        0
      ],
      "clip": [
        "9",
        1
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "21": {
    "inputs": {
      "model_name": "4x-UltraSharp.pth"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "Load Upscale Model"
    }
  },
  "22": {
    "inputs": {
      "upscale_model": [
        "21",
        0
      ],
      "image": [
        "13",
        0
      ]
    },
    "class_type": "ImageUpscaleWithModel",
    "_meta": {
      "title": "Upscale Image (using Model)"
    }
  },
  "24": {
    "inputs": {
      "upscale_method": "nearest-exact",
      "scale_by": 0.5,
      "image": [
        "22",
        0
      ]
    },
    "class_type": "ImageScaleBy",
    "_meta": {
      "title": "Upscale Image By"
    }
  },
  "42": {
    "inputs": {
      "prompt": "hair",
      "threshold": 0.3,
      "sam_model": [
        "43",
        0
      ],
      "grounding_dino_model": [
        "44",
        0
      ],
      "image": [
        "45",
        0
      ]
    },
    "class_type": "GroundingDinoSAMSegment (segment anything)",
    "_meta": {
      "title": "GroundingDinoSAMSegment (segment anything)"
    }
  },
  "43": {
    "inputs": {
      "model_name": "sam_vit_b_01ec64.pth",
      "device_mode": "Prefer GPU"
    },
    "class_type": "SAMLoader",
    "_meta": {
      "title": "SAMLoader (Impact)"
    }
  },
  "44": {
    "inputs": {
      "model_name": "GroundingDINO_SwinT_OGC (694MB)"
    },
    "class_type": "GroundingDinoModelLoader (segment anything)",
    "_meta": {
      "title": "GroundingDinoModelLoader (segment anything)"
    }
  },
  "45": {
    "inputs": {
      "image": "surprised-shocked-young-woman-standing-and-looking.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "human_image(SB)"
    }
  },
  "47": {
    "inputs": {
      "mask": [
        "42",
        1
      ]
    },
    "class_type": "MaskToImage",
    "_meta": {
      "title": "Convert Mask to Image"
    }
  },
  "50": {
    "inputs": {
      "prompt": "face",
      "threshold": 0.3,
      "sam_model": [
        "51",
        0
      ],
      "grounding_dino_model": [
        "52",
        0
      ],
      "image": [
        "45",
        0
      ]
    },
    "class_type": "GroundingDinoSAMSegment (segment anything)",
    "_meta": {
      "title": "GroundingDinoSAMSegment (segment anything)"
    }
  },
  "51": {
    "inputs": {
      "model_name": "sam_vit_b_01ec64.pth",
      "device_mode": "Prefer GPU"
    },
    "class_type": "SAMLoader",
    "_meta": {
      "title": "SAMLoader (Impact)"
    }
  },
  "52": {
    "inputs": {
      "model_name": "GroundingDINO_SwinT_OGC (694MB)"
    },
    "class_type": "GroundingDinoModelLoader (segment anything)",
    "_meta": {
      "title": "GroundingDinoModelLoader (segment anything)"
    }
  },
  "54": {
    "inputs": {
      "mask": [
        "50",
        1
      ]
    },
    "class_type": "MaskToImage",
    "_meta": {
      "title": "Convert Mask to Image"
    }
  },
  "109": {
    "inputs": {
      "x": 0,
      "y": 0,
      "operation": "add",
      "destination": [
        "42",
        1
      ],
      "source": [
        "50",
        1
      ]
    },
    "class_type": "MaskComposite",
    "_meta": {
      "title": "MaskComposite_ADD"
    }
  },
  "110": {
    "inputs": {
      "expand": 30,
      "tapered_corners": true,
      "mask": [
        "109",
        0
      ]
    },
    "class_type": "GrowMask",
    "_meta": {
      "title": "GrowMask +30"
    }
  },
  "111": {
    "inputs": {
      "expand": -30,
      "tapered_corners": true,
      "mask": [
        "110",
        0
      ]
    },
    "class_type": "GrowMask",
    "_meta": {
      "title": "GrowMask -30"
    }
  },
  "112": {
    "inputs": {
      "mask": [
        "111",
        0
      ]
    },
    "class_type": "InvertMask",
    "_meta": {
      "title": "InvertMask"
    }
  },
  "113": {
    "inputs": {
      "mask": [
        "128",
        0
      ]
    },
    "class_type": "MaskToImage",
    "_meta": {
      "title": "Convert Mask to Image"
    }
  },
  "115": {
    "inputs": {
      "pixels": [
        "117",
        0
      ],
      "vae": [
        "15",
        0
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "117": {
    "inputs": {
      "max_width": 768,
      "max_height": 768,
      "min_width": 0,
      "min_height": 0,
      "crop_if_required": "no",
      "images": [
        "45",
        0
      ]
    },
    "class_type": "ConstrainImage|pysssss",
    "_meta": {
      "title": "Constrain Image üêç"
    }
  },
  "120": {
    "inputs": {
      "samples": [
        "115",
        0
      ],
      "mask": [
        "128",
        0
      ]
    },
    "class_type": "SetLatentNoiseMask",
    "_meta": {
      "title": "Set Latent Noise Mask"
    }
  },
  "121": {
    "inputs": {
      "strength": 1,
      "start_percent": 0,
      "end_percent": 1,
      "positive": [
        "11",
        0
      ],
      "negative": [
        "12",
        0
      ],
      "control_net": [
        "122",
        0
      ],
      "image": [
        "123",
        0
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "Apply ControlNet (Advanced)"
    }
  },
  "122": {
    "inputs": {
      "control_net_name": "control_v11p_sd15_openpose_fp16.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "123": {
    "inputs": {
      "detect_hand": "enable",
      "detect_body": "enable",
      "detect_face": "enable",
      "resolution": 512,
      "bbox_detector": "yolox_l.onnx",
      "pose_estimator": "dw-ll_ucoco_384_bs5.torchscript.pt",
      "scale_stick_for_xinsr_cn": "disable",
      "image": [
        "117",
        0
      ]
    },
    "class_type": "DWPreprocessor",
    "_meta": {
      "title": "DWPose Estimator"
    }
  },
  "128": {
    "inputs": {
      "expand": -1,
      "tapered_corners": true,
      "mask": [
        "112",
        0
      ]
    },
    "class_type": "GrowMask",
    "_meta": {
      "title": "GrowMask -1"
    }
  },
  "129": {
    "inputs": {
      "seed": 504902085440580
    },
    "class_type": "Seed (rgthree)",
    "_meta": {
      "title": "Seed (rgthree)"
    }
  },
  "152": {
    "inputs": {
      "pixels": [
        "24",
        0
      ],
      "vae": [
        "15",
        0
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "153": {
    "inputs": {
      "x": 0,
      "y": 0,
      "resize_source": false,
      "destination": [
        "152",
        0
      ],
      "source": [
        "155",
        0
      ],
      "mask": [
        "158",
        0
      ]
    },
    "class_type": "LatentCompositeMasked",
    "_meta": {
      "title": "LatentCompositeMasked"
    }
  },
  "154": {
    "inputs": {
      "max_width": 1536,
      "max_height": 1536,
      "min_width": 0,
      "min_height": 0,
      "crop_if_required": "no",
      "images": [
        "45",
        0
      ]
    },
    "class_type": "ConstrainImage|pysssss",
    "_meta": {
      "title": "Constrain Image üêç"
    }
  },
  "155": {
    "inputs": {
      "pixels": [
        "154",
        0
      ],
      "vae": [
        "15",
        0
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "156": {
    "inputs": {
      "samples": [
        "153",
        0
      ],
      "vae": [
        "15",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "158": {
    "inputs": {
      "expand": -1,
      "tapered_corners": true,
      "mask": [
        "111",
        0
      ]
    },
    "class_type": "GrowMask",
    "_meta": {
      "title": "GrowMask -1"
    }
  },
  "159": {
    "inputs": {
      "filename_prefix": "HPD",
      "images": [
        "156",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  }
}